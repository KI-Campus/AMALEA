{"cells":[{"cell_type":"code","execution_count":null,"source":["# [Nur Colab] Diese Zellen müssen nur auf *Google Colab* ausgeführt werden und installieren Packete und Daten\n","!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n","!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["# Schöne Nachbarschaft"],"metadata":{}},{"cell_type":"markdown","source":["## Importe"],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import pandas as pd\n","import numpy as np\n","\n","%matplotlib inline\n","\n","import matplotlib.pyplot as plt"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## K-Nächste Nachbarn (KNN) (engl. K-Nearest-Neighbors)\n","\n","### Was sind KNN ?\n","\n","Lassen Sie uns zunächst ein paar Definitionen und Notationen festhalten. Im Folgenden werden wir $x$ zur Bezeichnung eines *Features* (auch Prädiktor, Attribut) und $y$ zur Bezeichnung des *Targets* (auch Label, Klasse) verwenden, welches wir vorhersagen wollen.\n","\n","KNN gehört zur Familie der Algorithmen für **überwachtes Lernen**. Informell bedeutet dies, dass wir einen gelabelten Datensatz erhalten, der aus Trainingsbeobachtungen $(x,y)$ besteht, und wir möchten die Beziehung zwischen $x$ und $y$ erfassen. Formaler ausgedrückt: Unser Ziel ist es, eine Funktion $h:X→Y$ zu lernen, so dass bei einer Beobachtung $x, h(x)$ die entsprechende Ausgabe $y$ sicher vorhersagen kann.\n","\n","### Einführung\n","\n","In der Klassifizierungsumgebung des K-Nächsten-Nachbarn-Algorithmus wird eine Mehrheitsabstimmung zwischen den K-ähnlichsten Instanzen zu einer gegebenen \"ungesehenen\" Beobachtung gebildet. Die Ähnlichkeit lässt sich anhand einer Abstandsmetrik zwischen zwei Datenpunkten definieren. Die als $x = <a_1(x),a_2(x),...,a_n(x)>$ gegebenen Instanzen werden als Punkte im n-dimensionalen Raum $\\mathbb{R}^n$ dargestellt. Ihre Beziehungen/Distanzen können folgendermaßen formuliert werden:\n","\n","\\begin{equation}\n","d(x_i,x_j) \\equiv \\sqrt{\\sum_{r=0}^{n-1} (a_r(x_i) - a_r(x_j))^2}\n","\\end{equation}\n","\n","In diesem Fall wird als Abstandsmetrik der euklidische Abstand gewählt, welcher eine gängige Wahl ist. Andere Abstandsmetriken können für eine bestimmte Umgebung besser geeignet sein, z. B. der Manhatten-, Tschebyscheff- und Hamming-Abstand. \n","\n","Eine Funktion $h$ wird dann aus $\\mathbb{R}^n$ $\\rightarrow$ $V$ gelernt, wobei $V$ eine endliche Menge aller möglichen Klassen darstellt. \n","\n","#### Algorithmus\n","\n","Jede gegebene Instanz $x_i$ wird zur Liste der Trainingsbeispiele hinzugefügt.  \n","\n","**Schlussfolgerung:**\n","\n","Eine neue Instanz $x_q$ muss klassifiziert werden und $x_1,...,x_k$ sind die $k$ nächstgelegenen Instanzen zu $x_q$ (diese sind nach Berechnung aller Abstände zwischen Trainingsbeispielen und der neuen Instanz bekannt). Beachten Sie, dass $K$ normalerweise ungerade ist, um Gleichstandssituationen zu vermeiden.\n","Folglich ergibt sich die neue Klassifikation von $x_q$ durch:\n","\n","\\begin{equation}\n","h(x_q) \\leftarrow arg\\max_{v \\in V}\\sum_{i=1}^{k}\\delta (v,y_i)\n","\\end{equation}\n","\n","mit \n","\\begin{equation}\n","\\delta (a,b) = \\begin{cases}\n","1, \\text{if } a=b \\\\\n","0, else\n","\\end{cases}\n","\\end{equation}\n"," \n","#### Beispiel: \n","\n","Klassifikation mit $K=5$:\n","\n","<img src=\"images/knn_5.png\" width=\"800\">\n","\n","__$\\rightarrow$Die neue Instanz wird als Quadrat klassifiziert.__\n","\n","Klassifikation mit $K=1$:\n","<img src=\"images/knn_1.png\" width=\"800\">\n","\n","__$\\rightarrow$Die neue Instanz wird als Dreieck klassifiziert.__ \n","\n","\n","\n","\n","Wenn $k = 1$ ist, sind die Entscheidungsgrenzen letztendlich die gleichen wie in einem [Voronoi](https://en.wikipedia.org/wiki/Voronoi_diagram) Diagramm.  Während bei Kreisen der Blickpunkt darin besteht, die nächsten K nächstgelegenen Punkte zur neuen Instanz zu finden, zeigt ein Voronoi-Diagramm Regionen, in denen neue Instanzen den bekannten Datenpunkten zugeordnet werden. (Der Blickpunkt liegt nun auf den Quadraten und Dreiecken, die ihrer Klasse neue Punkte neben sich zuordnen).\n","\n","**Zusätzliche Information:**\n","\n","- Im Allgemeinen ist es oft sinnvoll, die Input Vektoren zu normalisieren, damit die Dimensionen des Inputs nicht so stark verzerrt werden.\n","\n","- Es besteht die Möglichkeit, abstandsbasierte Gewichte für Instanzen zu verwenden, anstatt einheitlich bekannte Instanzen zu berücksichtigen. Daher ändert sich die oben angegebene Gleichung in:\n","\n","    \\begin{equation}\n","    f(x_q) \\leftarrow arg\\max_{v \\in V}\\sum_{i=1}^{k} w_i \\delta (v,c(x_i))\n","    \\end{equation}\n","\n","    und die Gewichte sind gegeben durch:\n","\n","    \\begin{equation}\n","     w_i \\equiv \\frac{1}{d(x_q,x_i)^2}\n","    \\end{equation}\n","    \n","- __Cover und Hart 1967__: Mit $n \\rightarrow \\infty$, ist der 1-NN-Fehler nicht mehr als doppelt so groß wie der Fehler des Bayes Optimal Klassifikators.(Analog für k>1.) [Quelle](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)\n","\n","- __Fluch der Dimensionalität__: Der induktive Bias des K-NN-Algorithmus besteht darin, dass ähnliche Punkte Labels teilen. In hochdimensionalen Räumen trifft diese Annahme nicht mehr so gut wie in niederdimensionalen Räumen. Dies liegt daran, dass Punkte nicht nahe beieinander liegen, wenn sie in jeder Dimension gleichmäßig liegen. Siehe auch: [Quelle](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)\n","\n","- Reduzieren Sie hochdimensionale Daten durch PCA oder SVD. Anschließend sollten die intrinsischen Dimensionen der Daten niedriger dimensioniert sein oder diese nutzen nicht alle Dimensionen, in denen die Daten vorliegen\n","\n","- K-NN-Algorithmus wird langsam, wenn n oder die Dimensionen d zunehmen. Dies könnte zu einer nicht durchführbaren Inferenzzeit führen\n","\n","- K-NN-Algorithmus wird mit zunehmendem n immer genauer"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 3.3.1:</b> Laden Sie die vorbereiteten Training- und Testdatensätze, da wir nur GridSearchCV verwenden werden\n","\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["# Load the prepared datasets training and test set, because we are only going to use GridSearchCV\n","\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 3.3.2:</b> Normalisieren Sie die Daten mit MinMaxScaling. Bitte verwenden Sie die Formel und achten Sie darauf, dass der Datentyp aufgrund der Visualisierung später in diesem Notebook noch ein pandas Dataframe ist__(Benutzen Sie nicht den MinMaxScaler!)__. Achten Sie ebenso auf identische Variablenbezeichnungen und mehrfache Zellenausführung.\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["# Normalize the data\n","\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["# Imports the necessary modules\n","\n","from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n","from matplotlib.colors import ListedColormap\n","num_neighbors = 1\n","radius = 100"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Die k-Nachbarn-Klassifikation in KNeighborsClassifier ist die am häufigsten verwendete Technik. Die optimale Wahl des Wertes ist stark datenabhängig: Im Allgemeinen unterdrückt ein größeres k die Auswirkungen von Rauschen, reduziert allerdings die Deutlichkeit der Klassifizierungsgrenzen.\n","\n","In Fällen, in denen die Daten nicht gleichmäßig verteilt sind, kann die radiusbasierte Nachbarschaftsklassifizierung in RadiusNeighborsClassifier eine bessere Wahl sein. Hier gibt der/die NutzerIn einen festen Radius vor, so dass Punkte in spärlicheren Nachbarschaften eine geringere Anzahl an nächsten Nachbarn für die Klassifizierung verwenden. Für hochdimensionale Parameterräume wird diese Methode aufgrund des sogenannten \"Fluch der Dimensionalität\" weniger effektiv.\n","\n","Die grundlegende Klassifikation der nächsten Nachbarn verwendet einheitliche Gewichtungen: Das heißt, der einem Abfragepunkt zugewiesene Wert wird aus einer einfachen Mehrheitsentscheidung der nächsten Nachbarn berechnet. Unter Umständen ist es besser, die Nachbarn so zu gewichten, dass nähere Nachbarn mehr zur Anpassung beitragen. Dies kann durch das Schlüsselwort weights erreicht werden. Der Standardwert weights = 'uniform' weist jedem Nachbarn einheitliche Gewichte zu. weights = 'distance' weist Gewichtungen proportional zum Kehrwert der Entfernung vom Abfragepunkt zu. Alternativ kann eine benutzerdefinierte Funktion des Abstands geliefert werden, um die Gewichte zu berechnen.\" [Quelle](https://scikit-learn.org/stable/modules/neighbors.html)"],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["neigh = KNeighborsClassifier(n_neighbors=num_neighbors)\n","neigh.fit(x_train, y_train)\n","\n","neigh_r = RadiusNeighborsClassifier(radius)\n","neigh_r.fit(x_train, y_train)\n","\n","print(\"Test score of K-Nearest Neighbor: %f\" %neigh.score(x_test, y_test))\n","print(\"Test score of K-Nearest Neighbor with radius: %f\" %neigh_r.score(x_test, y_test))"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Optimierung"],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["from sklearn.model_selection import GridSearchCV"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Optimale Anzahl an Nachbarn für den konventionellen Klassifikator (KNN) (engl. Conventional Classifier)\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 3.3.3:</b> \n","<ul>\n","<li> Optimieren Sie nicht nur in Bezug auf einen, sondern auf zwei Parameter\n","<li> Optimieren Sie k in nearest_n_arr sowie die Gewichtsverteilung.\n","<li> Nutzen Sie GridSearchCV und einen Dictionary für k_params (in der KNN-Aufgabe müssen Sie nicht StratifiedKfold verwenden, sondern nur den Parameter cv, der eine stratifizierte Faltung durchführt, aber ohne den Parameter random_state.)\n","\n","</ul>\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["# Nearest Neighbors values to optimize the model\n","nearest_n_arr = range(1,20)  # natural numbers\n","\n","# Necessary for plotting\n","length_a = len(nearest_n_arr)  \n","\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["neighourhood = k_params['n_neighbors']\n","scores = k_model.cv_results_['mean_test_score']\n","scores_std = k_model.cv_results_['std_test_score']\n","\n","# Resorting for plots\n","scores = np.append(scores[0::2], scores[1::2])\n","scores_std = np.append(scores_std[0::2], scores_std[1::2])\n","\n","neighourhood = k_params['n_neighbors']\n","\n","plt.figure().set_size_inches(20, 5)\n","plt.xlabel('Number of Neighbors')\n","plt.xlim(neighourhood[0],neighourhood[-1])\n","plt.ylabel('Mean Test Score')\n","plt.plot(neighourhood, scores[:length_a],'g--',\n","        label='uniform')\n","plt.plot(neighourhood, scores[length_a:]\n","         , 'b--',label='distance')\n","plt.title('Optimal Number of Neighbors w.r.t. weight distribution')\n","plt.legend()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Das beste Modell erhalten\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 3.3.4:</b> \n","<ul>\n","<li> Ermitteln Sie die besten Parameter und die zugehörigen Ergebnisse.\n","<li> Geben Sie diese aus! \n","</ul>\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["# Get the params, and the test score of the best model, print the values\n","\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Optimaler Radius für den Radius Klassifikator (engl. RadiusClassifier)\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 3.3.5:</b> \n","<ul>\n","<li> Analog wie zuvor: Benennen Sie Ihren GridSearchCV k_r_model und verwenden Sie k_r_params (Sehen Sie sich den nächsten Code-Block an, um Konsistenz zu gewährleisten)\n","<li> Optimieren Sie nun den Dictionary mit Radius und Gewichten\n","<li> Sie können nun entscheiden, ob Sie Dezimalschritte oder natürliche Zahlen für die Radien wünschen.\n","</ul>\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["# Choose for different radii\n","radius_arr = np.arange(0.6, 1.2, 0.1)  # decimal steps\n","radius_arr = range(1,10)  # natural numbers\n","\n","# Necessary for plotting\n","length_r_a = len(radius_arr)  \n","\n","\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["neighourhood_r = k_r_params['radius']\n","scores_r = k_r_model.cv_results_['mean_test_score']\n","scores_std_r = k_r_model.cv_results_['std_test_score']\n","\n","scores_r = np.append(scores_r[0::2], scores_r[1::2])\n","scores_std_r = np.append(scores_std_r[0::2], scores_std_r[1::2])\n","\n","plt.figure().set_size_inches(20, 5)\n","plt.title('Optimal Radius w.r.t. weight distribution')\n","plt.xlim(neighourhood_r[0],neighourhood_r[-1])\n","plt.xlabel('Radius')\n","plt.ylabel('Mean Test Score')\n","plt.plot(neighourhood_r, scores_r[:length_r_a],'g--',\n","        label='uniform')\n","plt.plot(neighourhood_r, scores_r[length_r_a:],'b--',\n","        label='distance')\n","plt.legend()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.6:</b> Was passiert, wenn Sie einen Radius von 0 nehmen?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.7\n","    :</b> Können Sie sich vorstellen, warum die Korrektklassifikationsrate (engl. accuracy) des Modells ab einem bestimmten Radius stagniert?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 3.3.8:</b>  Ändern Sie das Radius-Array von nur natürlichen Zahlen auf Dezimalzahlen wie 0.6, 0.7.,... (kommentieren Sie die Zeile mit den natürlichen Zahlen aus). Beachten Sie, dass für einige Radien keine Nachbarn gefunden werden können. Passen Sie daher die untere Grenze an, bis mindestens ein Nachbar gefunden wird.\n","</li>\n","</ul>\n","</div>"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.9:</b> Warum macht der zweite Ansatz mehr Sinn?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["#### Das beste Modell erhalten\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 3.3.10:</b> Ermitteln Sie für den Radius Klassifikator das beste Modell und dessen entsprechende Parameter. Geben Sie diese mit dem Score aus.\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["# Get the params, and the test score of the best model, print the values\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Vergleich"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.11:</b> Welches Modell würden Sie nehmen, Radius Klassifikator (engl.RadiusClassifier) oder konventioneller Klassifikator (engl. Conventional Classifier) und warum? Vergleichen Sie daher die trainierten Klassifikatoren mit der besten Parametereinstellung und die Ergebnisse von GridSearchCV.\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.12:</b> Sind die Gewichte in den zwei besten Modellen einheitlich?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["### Visualisierung\n","\n","Wir nehmen die beiden Features des Trainingssatzes und schauen, wo die Entscheidungsgrenzen für ein darauf trainiertes KNN gezogen werden."],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["def plot_knn(n_neighbors:int, radius:float, f_names:list, X:np.array, y:np.ndarray):\n","    \n","    # Create color maps\n","    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n","    cmap_bold = ListedColormap(['#FF0000','#0000FF'])\n","    \n","    h = .02\n","    for weights in ['uniform', 'distance']:\n","        # we create an instance of Neighbours Classifier and fit the data.\n","        clf = KNeighborsClassifier(n_neighbors, weights=weights)\n","        clf_r = RadiusNeighborsClassifier(radius=radius, weights= weights, outlier_label=0)\n","        clf.fit(X, y)\n","        clf_r.fit(X,y)\n","        \n","        # Plot the decision boundary. For that, we will assign a color to each\n","        # point in the mesh [x_min, x_max]x[y_min, y_max].\n","        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                             np.arange(y_min, y_max, h))\n","        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","        Z2 = clf_r.predict(np.c_[xx.ravel(), yy.ravel()])\n","        \n","        # Put the result into a color plot\n","        Z = Z.reshape(xx.shape)\n","        Z2 = Z2.reshape(xx.shape)\n","         \n","        f,ax = plt.subplots(1,2,figsize=(20,5))\n","        \n","        ax[0].pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n","        sc1 = ax[0].scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n","                    edgecolor='k', s=20)\n","\n","        ax[0].set_xlim(xx.min(), xx.max())\n","        ax[0].set_ylim(yy.min(), yy.max())\n","        ax[0].set_xlabel(f_names[0])\n","        ax[0].set_ylabel(f_names[1])\n","        ax[0].set_title(\"K-Nearest Neighbor with (k= %i, weights = '%s')\"\n","                       % (n_neighbors, weights))\n","        f.colorbar(sc1,ax = ax[0]) \n","        \n","        ax[1].pcolormesh(xx, yy, Z2, cmap=cmap_light, shading='auto')\n","        sc2 = ax[1].scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n","                    edgecolor='k', s=20)\n","        \n","        ax[1].set_xlim(xx.min(), xx.max())\n","        ax[1].set_ylim(yy.min(), yy.max())\n","        ax[1].set_xlabel(f_names[0])\n","        ax[1].set_ylabel(f_names[1])\n","        ax[1].set_title(\"Radius Neighbors Classifier with (R= %0.3f, weights = '%s')\"\n","                       % (radius, weights))\n","        f.colorbar(sc1, ax = ax[1]) \n","       \n","    plt.show()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["# Reconsider all features we have\n","x_test.columns"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["# Choose features to compare\n","features_compared = ['Pclass', 'Sex']\n","\n","# Plot them\n","plot_knn(2, 0.3, features_compared, np.array(x_test[features_compared]), y_test.values)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.13:</b> Welche Funktionen wurden in der Visualisierung verglichen?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.14:</b> Warum sind in der Visualisierung nur 6 Punkte vorhanden?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwortr:</b> Da es für jedes Feature nur 3 bzw. 2 Werte gibt, ergeben sich 6 Kombinationen.\n","</div>"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.15:</b> Sind die zuvor ermittelten optimalen Werte für KNN und RNN (über GridSearchCV) für diese Plots zuverlässig?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.16:</b>  Was passiert mit den Entscheidungsgrenzen, wenn Sie k erhöhen? Sie könnten die Features ändern, um das zu sehen.\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 3.3.17:</b> Plotten Sie stetige Features, wenn möglich.\n","</div>"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 3.3.18:</b> Lassen Sie die Normalisierung gleich zu Beginn dieser Aufgabe weg.\n","</div>"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.19:</b>  Ist es eine gute Idee, nicht zu normalisieren?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["### Visualisierung mit verschiedenen Methoden\n","\n","Im Folgenden werden einige Möglichkeiten zur Visualisierung von Daten in höheren Dimensionen verwendet, da wir mit den knn_plots bisher nur zwei Dimensionen betrachtet haben."],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","from sklearn.decomposition import PCA as sklearnPCA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.datasets import make_blobs\n","\n","from pandas.plotting import parallel_coordinates"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Dimensionsreduktion mit PCA\n","\n","Die Dimensionsreduktion mit PCA kann auch für Preprocessing Zwecke verwendet werden (auch mit K-NNs). Diese wird hier aber nur für Visualisierungszwecke verwendet."],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["pca = sklearnPCA(n_components=2) #2-dimensional PCA\n","transformed = pd.DataFrame(pca.fit_transform(x_train))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["plt.figure(figsize=(20,5))\n","plt.scatter(transformed[y_train==0][0], transformed[y_train==0][1], label='Nicht überlebt', c='red')\n","plt.scatter(transformed[y_train==1][0], transformed[y_train==1][1], label='Überlebt', c='green')\n","plt.title(\"PCA Plot der Titanic-Features\")\n","\n","plt.legend()\n","plt.show()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Parallele Koordinaten\n","\n","Eine Zeile beschreibt einen Datenpunkt in Ihren Daten. Wenn Sie sich \"stetige\" Werte ansehen (mehr als nur eine Handvoll diskreter Werte), können Sie sehen, dass zum Beispiel lange Namen dazu tendieren, zu überleben."],"metadata":{}},{"cell_type":"code","execution_count":19,"source":["data_norm = pd.concat([x_train, y_train], axis=1)\n","\n","# Perform parallel coordinate plot\n","plt.figure(figsize=(20,5))\n","parallel_coordinates(data_norm, 'Survived')\n","plt.show()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 3.3.20:</b>  Was passiert, wenn die Daten bei diesen Methoden nicht normalisiert werden? Prüfen Sie selbst.\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["Siehe auch: http://www.apnorton.com/blog/2016/12/19/Visualizing-Multidimensional-Data-in-Python/"],"metadata":{}},{"cell_type":"markdown","source":["## Weitere Methoden\n","\n","\n","- Bagging\n","- Logistic Regression\n","- Random Forests\n","- Kombination aus Bagging und GridSearchCV"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python-amalea"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":4}
